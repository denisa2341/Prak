{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.04, max_features=None, min_df=10,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=frozenset({'other', 'becomes', 'four', 'go', 'has', 'thick', 'often', 'system', 'by', 'cant', 'done', 'her', 'too', 'hereby', 'was', 'nevertheless', 'will', 'why', 'everything', 'seem', 'while', 'may', 'together', 'call', 'even', 'either', 'thru', 'in', 'every', 'between', 'fill', 'per', ...', 'yet', 'their', 'not', 'well', 'and', 'us', 'about', 'have', 'do', 'else', 'almost', 'we', 'eg'}),\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words=ENGLISH_STOP_WORDS,analyzer='word', binary=True, min_df=10, max_df=.04)\n",
    "vectorizer.fit(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 10299)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def fun(X, number, a, b, n_iter=10):\n",
    "\n",
    "    m1 = np.zeros((number, X.shape[1]))\n",
    "    m2 = np.zeros((X.shape[0], number))\n",
    "    m3 = np.zeros(number)\n",
    "    print(\"*\")\n",
    "    docs, words = X.nonzero()\n",
    "    z = np.random.choice(number, len(docs))\n",
    "    \n",
    "    for doc, word, z_ in zip(docs, words, z):\n",
    "        m2[doc, z_] += 1\n",
    "        m1[z_, word] += 1\n",
    "        m3[z_] += 1\n",
    "    \n",
    "    for iter_ in tqdm(range(n_iter)):\n",
    "        for i in range(len(docs)):\n",
    "            m2[docs[i], z[i]] -= 1\n",
    "            m1[z[i], words[i]] -= 1\n",
    "            m3[z[i]] -= 1\n",
    "            \n",
    "            param = (m2[docs[i], :] + a) * (m1[:, words[i]] + b[words[i]]) / \\\n",
    "                (m3 + b.sum())\n",
    "            z[i] = np.random.choice(np.arange(number), p=param / param.sum())\n",
    "            \n",
    "            m2[docs[i], z[i]] += 1\n",
    "            m1[z[i], words[i]] += 1\n",
    "            m3[z[i]] += 1\n",
    "    \n",
    "    return z, m1, m2, m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  2%|▎         | 1/40 [02:38<1:43:11, 158.75s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  5%|▌         | 2/40 [05:38<1:44:27, 164.93s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  8%|▊         | 3/40 [08:46<1:45:57, 171.82s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 10%|█         | 4/40 [11:24<1:40:41, 167.81s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 12%|█▎        | 5/40 [14:28<1:40:40, 172.58s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 15%|█▌        | 6/40 [17:19<1:37:29, 172.06s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 18%|█▊        | 7/40 [20:09<1:34:20, 171.52s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 20%|██        | 8/40 [23:59<1:40:56, 189.26s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 22%|██▎       | 9/40 [27:17<1:39:04, 191.77s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 25%|██▌       | 10/40 [30:19<1:34:20, 188.70s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 28%|██▊       | 11/40 [33:42<1:33:23, 193.22s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 30%|███       | 12/40 [37:32<1:35:16, 204.16s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 32%|███▎      | 13/40 [41:00<1:32:22, 205.29s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 35%|███▌      | 14/40 [43:49<1:24:13, 194.37s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 38%|███▊      | 15/40 [46:48<1:19:07, 189.89s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 40%|████      | 16/40 [49:39<1:13:39, 184.16s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 42%|████▎     | 17/40 [52:17<1:07:37, 176.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 45%|████▌     | 18/40 [55:15<1:04:45, 176.62s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 48%|████▊     | 19/40 [58:09<1:01:36, 176.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 20/40 [1:00:46<56:48, 170.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 52%|█████▎    | 21/40 [1:03:24<52:47, 166.69s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████▌    | 22/40 [1:06:42<52:46, 175.90s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 57%|█████▊    | 23/40 [1:09:30<49:12, 173.68s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████    | 24/40 [1:12:13<45:27, 170.44s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 62%|██████▎   | 25/40 [1:15:12<43:15, 173.05s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 65%|██████▌   | 26/40 [1:18:00<39:59, 171.39s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 68%|██████▊   | 27/40 [1:20:27<35:32, 164.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████   | 28/40 [1:23:41<34:37, 173.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████▎  | 29/40 [1:26:49<32:31, 177.42s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 75%|███████▌  | 30/40 [1:30:39<32:12, 193.26s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 78%|███████▊  | 31/40 [1:33:51<28:57, 193.03s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 80%|████████  | 32/40 [1:37:20<26:21, 197.71s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 82%|████████▎ | 33/40 [1:40:12<22:10, 190.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████▌ | 34/40 [1:43:15<18:46, 187.80s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 88%|████████▊ | 35/40 [1:46:57<16:31, 198.24s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 90%|█████████ | 36/40 [1:49:58<12:52, 193.15s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 92%|█████████▎| 37/40 [1:52:46<09:16, 185.50s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 95%|█████████▌| 38/40 [1:55:42<06:05, 182.63s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████▊| 39/40 [1:58:23<02:56, 176.22s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 40/40 [2:01:21<00:00, 176.74s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "number = 20\n",
    "z, m1, m2, m3 = fun(X_train, number, 1 * np.ones(number), 1 * np.ones(X_train.shape[1]), 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tag 1:\t1993\tapril\tclinton\thouse\tincluding\tnational\tpresident\tstates\ttoday\twhite\n",
      " Tag 2:\tchip\tclipper\tencryption\tkey\tkeys\tphone\tpublic\tsecret\tsecure\tsecurity\n",
      " Tag 3:\tcard\tcomputer\tdisk\tdos\thi\tmac\tmemory\tmonitor\tpc\tvideo\n",
      " Tag 4:\tanswer\tanybody\tarticle\tcurrent\tobvious\tshort\tsmall\tsorry\tvoltage\twondering\n",
      " Tag 5:\t100\tasking\tcondition\tcontact\toffer\tphone\tprice\tsale\tsell\tshipping\n",
      " Tag 6:\tchildren\tcountry\thistory\tisrael\tisraeli\tjews\tkilled\tland\ttoday\twar\n",
      " Tag 7:\tcenter\tearth\tlaunch\tlow\tnasa\torbit\tproject\tresearch\tscience\tspace\n",
      " Tag 8:\tadvance\tanybody\tguess\tguys\thaven\they\tlevel\tok\tsimple\tsounds\n",
      " Tag 9:\tapplication\tcode\tfile\tfiles\tftp\tgraphics\tserver\tuser\tversion\twindow\n",
      " Tag 10:\tbanks\tcause\tdoctor\teffect\tgordon\tmedical\tpitt\tsoon\tsurrender\tusually\n",
      " Tag 11:\tcheers\tcomes\texactly\tideas\tleave\tlonger\tnewsgroup\tok\tsound\tworks\n",
      " Tag 12:\tappreciated\tbtw\tdeleted\tgets\thear\they\toh\tsimply\tstuff\tthinking\n",
      " Tag 13:\t11\t12\t13\t14\t17\t18\t21\t23\t24\t25\n",
      " Tag 14:\tbike\tbought\tcar\tcars\tengine\tfast\tleft\tmiles\troad\tturn\n",
      " Tag 15:\tcouldn\tcouple\tfood\tguess\thome\tlooked\tsorry\tstarted\tstuff\twait\n",
      " Tag 16:\tamerican\tcontrol\tcrime\tgun\tguns\tlaw\tlaws\trights\tself\tweapons\n",
      " Tag 17:\tca\tdavid\tinternet\tmichael\tnet\tposting\tpostings\ttoday\ttom\tuniversity\n",
      " Tag 18:\tgame\tgames\thockey\tleague\tplay\tplayers\tseason\tteam\tteams\twin\n",
      " Tag 19:\tagree\tbible\tchrist\tchristian\tchristians\tclaim\tjesus\tman\treligion\tword\n",
      " Tag 20:\tappreciate\tinstead\tnews\tpaper\tposting\tresult\tsorry\ttexas\tthank\twouldn\n"
     ]
    }
   ],
   "source": [
    "top_words = np.argsort(m1, axis=1)[:, :-11:-1]\n",
    "\n",
    "for tag in range(20):\n",
    "    doc = np.zeros((1, X_train.shape[1]))\n",
    "    for word in top_words[tag]:\n",
    "        doc[0, word] = 1\n",
    "    print(' Tag {}:\\t{}'.format(tag+1, '\\t'.join(vectorizer.inverse_transform(doc)[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.target_names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
